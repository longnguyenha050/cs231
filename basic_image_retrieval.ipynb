{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from fastai import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback import *\n",
    "from fastai.data.transforms import get_image_files\n",
    "import pandas as pd\n",
    "from arch import RingGeMNet, GeMNet, L2Norm, GeM\n",
    "import re\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./index -> points to index dir\n",
    "COMP_DATA_DIR = Path('.')\n",
    "\n",
    "df = pd.DataFrame({'Image' : sorted(get_image_files(COMP_DATA_DIR / 'index', recurse=True))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS=8\n",
    "SIZE = 256\n",
    "DO_FULL_SIZE = False \n",
    "\n",
    "item_tfms = None if DO_FULL_SIZE else Resize(SIZE, method='squish')  # Biến đổi kích thước hình ảnh\n",
    "batch_tfms = Normalize.from_stats(*imagenet_stats)  # Chuẩn hóa ảnh với ImageNet\n",
    "\n",
    "# Định nghĩa DataBlock không chia tập dữ liệu\n",
    "data_block = DataBlock(\n",
    "    blocks=(ImageBlock(),),  # Chỉ dùng ImageBlock, không có nhãn\n",
    "    get_x=lambda row: row['Image'],  # Trả về đường dẫn ảnh dưới dạng chuỗi\n",
    "    splitter=RandomSplitter(valid_pct=0),  # Không chia tập dữ liệu\n",
    "    item_tfms=item_tfms,  # Phép biến đổi kích thước\n",
    "    batch_tfms=batch_tfms  # Chuẩn hóa ảnh\n",
    ")\n",
    "\n",
    "# Tạo DataLoaders\n",
    "BS = 1 if DO_FULL_SIZE else 64\n",
    "data = data_block.dataloaders(df, bs=BS, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Hiển thị một batch hình ảnh\n",
    "data.show_batch(max_n=9, figsize=(8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra thông tin của DataLoader\n",
    "print(\"Batch size:\", data.bs)\n",
    "print(\"Number of training samples:\", len(data.train_ds))\n",
    "print(\"Number of validation samples:\", len(data.valid_ds) if hasattr(data, 'valid_ds') else 0)\n",
    "print(\"Number of training batches:\", len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra các thuộc tính của DataLoaders\n",
    "print(data)\n",
    "\n",
    "# Kiểm tra batch đầu tiên trong train\n",
    "for batch in data.train:\n",
    "    print(batch)  # In ra batch đầu tiên\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = partial(pretrainedmodels.se_resnet152, num_classes=1000) \n",
    "\n",
    "arch.__name__ = arch.func.__name__\n",
    "model_fname =  None #'resnet152_i200_l1000-256'\n",
    "basename_suffix = 'cut-extractor-2scales6patches-gem3'\n",
    "size_fname = 'full' if DO_FULL_SIZE else str(SIZE)\n",
    "\n",
    "basename = f'{model_fname or arch.__name__}_{size_fname}_{basename_suffix}.pth'\n",
    "print(basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l2norm = L2Norm()\n",
    "        self.pool   = GeM(3.) #nn.AdaptiveMaxPool2d(1)\n",
    "    def forward(self, x):\n",
    "        b,d,ny,nx = x.shape\n",
    "        \n",
    "        f0  = self.l2norm(self.pool(x)).view(b,1,d)\n",
    "        # uncomment if you want to extract multiple patches here\n",
    "        \n",
    "        #f1  = self.l2norm(self.pool(x[...,ny//2-ny//4:ny//2+ny//4,nx//2-nx//4:nx//2+nx//4])).view(b,1,d)\n",
    "\n",
    "        #f1_x0y0 = self.l2norm(self.pool(x[...,:ny//2,:nx//2])).view(b,1,d)\n",
    "        #f1_x0y1 = self.l2norm(self.pool(x[...,ny//2:,:nx//2])).view(b,1,d)\n",
    "        #f1_x1y0 = self.l2norm(self.pool(x[...,:ny//2,nx//2:])).view(b,1,d)\n",
    "        #f1_x1y1 = self.l2norm(self.pool(x[...,ny//2:,nx//2:])).view(b,1,d)\n",
    "        \n",
    "        #return torch.cat((f0,f1,f1_x0y0,f1_x0y1,f1_x1y0,f1_x1y1), dim=1)\n",
    "        #return torch.cat((f0,f1), dim=1)\n",
    "        \n",
    "        return f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(data, arch,pretrained='imagenet', custom_head=Extractor(),\n",
    "                   metrics=[accuracy], cut= -1,\n",
    "                   loss_func=nn.CrossEntropyLoss(), n_out=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_fname:\n",
    "    learn = learn.load(model_fname, strict=False)\n",
    "else:\n",
    "    model_fname = arch.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InferenceNet =  learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS=8\n",
    "\n",
    "qdf = pd.DataFrame({'Image' : sorted(get_image_files(COMP_DATA_DIR / 'test', recurse=True))})\n",
    "qdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biến đổi kích thước hình ảnh\n",
    "item_tfms = Resize(SIZE, method='squish') if not DO_FULL_SIZE else None\n",
    "# Chuẩn hóa ảnh với ImageNet\n",
    "batch_tfms = Normalize.from_stats(*imagenet_stats)\n",
    "\n",
    "# Định nghĩa DataBlock\n",
    "data_block = DataBlock(\n",
    "    blocks=(ImageBlock(),),  # Chỉ dùng ImageBlock, không có nhãn\n",
    "    get_x=lambda row: row['Image'],  # Trả về đường dẫn ảnh dưới dạng chuỗi\n",
    "    splitter=RandomSplitter(valid_pct=0),  # Không chia tập dữ liệu\n",
    "    item_tfms=item_tfms,  # Phép biến đổi kích thước\n",
    "    batch_tfms=batch_tfms  # Chuẩn hóa ảnh\n",
    ")\n",
    "\n",
    "# Tạo DataLoaders\n",
    "BS = 1 if DO_FULL_SIZE else 64\n",
    "qdata = data_block.dataloaders(qdf, bs=BS, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Hiển thị một batch hình ảnh\n",
    "qdata.show_batch(max_n=9, figsize=(8, 8))\n",
    "\n",
    "# Nếu bạn muốn điều chỉnh bộ mẫu\n",
    "qdata.train.sampler = torch.utils.data.SequentialSampler(qdata.train_ds)\n",
    "qdata.train.drop_last = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra thông tin của DataLoader\n",
    "print(\"Batch size:\", qdata.bs)\n",
    "print(\"Number of training samples:\", len(qdata.train_ds))\n",
    "print(\"Number of validation samples:\", len(qdata.valid_ds) if hasattr(qdata, 'valid_ds') else 0)\n",
    "print(\"Number of training batches:\", len(qdata))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # import thư viện tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")  # Chỉ sử dụng CPU\n",
    "\n",
    "def extract_vectors_batched(data, model, flip=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    n_flip = 2 if flip else 1\n",
    "    n_img = len(data.train_ds) * n_flip\n",
    "    bs = data.bs\n",
    "    vectors = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Thêm tqdm vào vòng lặp for để theo dõi tiến trình\n",
    "        for idx, (img) in enumerate(tqdm(data.train, desc=\"Processing images\", unit=\"batch\")):\n",
    "            st = idx * bs * n_flip\n",
    "            fin = min((idx + 1) * bs * n_flip, n_img)\n",
    "            if flip:\n",
    "                img = torch.cat((img[0], img[0].flip([3])))\n",
    "            out = model(img).cpu()\n",
    "            if vectors is None:\n",
    "                vectors = torch.zeros(n_img, *out.shape[1:])\n",
    "            if flip:\n",
    "                n = fin - st\n",
    "                vectors[st:fin:2, ...] = out[:n // 2, ...]\n",
    "                vectors[st + 1:fin + 1:2, ...] = out[n // 2:, ...]\n",
    "            else:\n",
    "                vectors[st:fin, ...] = out\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = True\n",
    "p_flip = 'flip' if flip else ''\n",
    "try:\n",
    "    index_features = torch.load(( f'index{p_flip}_{basename}'), weights_only=True)\n",
    "except:\n",
    "    print(\"Tệp tin không tồn tại hoặc bị lỗi, đang tạo lại index_features.\")\n",
    "    index_features = extract_vectors_batched(data, InferenceNet, flip)\n",
    "    torch.save(index_features, f'index{p_flip}_{basename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = True\n",
    "p_flip = 'flip' if flip else ''\n",
    "try:\n",
    "    query_features = torch.load(( f'query{p_flip}_{basename}'),weights_only=True)\n",
    "except:\n",
    "    print(\"Tệp tin không tồn tại hoặc bị lỗi, đang tạo lại query_features.\")\n",
    "    query_features = extract_vectors_batched(qdata,InferenceNet, flip)\n",
    "    torch.save(query_features, f'query{p_flip}_{basename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "def flatten(list2d): return list(itertools.chain(*list2d))\n",
    "\n",
    "# duplicate b/c we're going to have image, image_LR, image, image_LR, ...\n",
    "query_fnames = flatten([[x.stem, x.stem] for x in qdf.Image.tolist()])\n",
    "index_fnames = [x.stem for x in df.Image.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "learn, InferenceNet, co, res, flat_config, cpu_index, index = None, None, None, None, None, None, None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_pcawhitenlearn(X):\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Learning PCA w/o annotations\n",
    "    m = X.mean(dim=0, keepdim=True)\n",
    "    Xc = X - m\n",
    "    Xcov = Xc.t() @ Xc\n",
    "    Xcov = (Xcov + Xcov.t()) / (2*N)\n",
    "    eigval, eigvec = torch.symeig(Xcov,eigenvectors=True)\n",
    "    order = eigval.argsort(descending=True)\n",
    "    eigval = eigval[order]\n",
    "    eigvec = eigvec[:, order]\n",
    "\n",
    "    P = torch.inverse(torch.sqrt(torch.diag(eigval))) @ eigvec.t()\n",
    "    \n",
    "    return m, P\n",
    "\n",
    "def t_whitenapply(X, m, P, dimensions=None):\n",
    "    \n",
    "    if not dimensions: dimensions = P.shape[1]\n",
    "\n",
    "    X = (X-m) @ P[:,:dimensions]\n",
    "    X = X / (torch.norm(X, dim=1, keepdim=True) + 1e-6)\n",
    "    return X\n",
    "\n",
    "def get_idxs_and_dists(_query_features, _index_features, index_type='', BS = 32):\n",
    "    \n",
    "    # if I do PCA and whitenining here I get different results than if doing it by faiss, why?\n",
    "    # hence I had to disable it and use faiss which resorts to CPU then (slower)\n",
    "    if False:\n",
    "        index_transforms = []\n",
    "        for index_transform in index_type.split(','):\n",
    "            m = re.match(r'PCAW(\\d+)?', index_transform)\n",
    "            if m is not None:\n",
    "                dimensions = int(m[1]) if m[1] is not None else _index_features.shape[-1]\n",
    "                print(f\"Applying {dimensions} PCA, Whitening and L2Norm...\", end=\"\")\n",
    "                m, P = t_pcawhitenlearn(_index_features)\n",
    "                _index_features = t_whitenapply(_index_features, m, P,dimensions=dimensions).unsqueeze(1)\n",
    "                _query_features = t_whitenapply(_query_features, m, P,dimensions=dimensions).unsqueeze(1)\n",
    "                print(\"done\")\n",
    "\n",
    "            elif index_transform not in ['L2norm']: index_transforms.append(index_transform)\n",
    "\n",
    "        index_type = ','.join(index_transforms)\n",
    "        print(index_type)\n",
    "    else:\n",
    "         _index_features = _index_features.unsqueeze(1)\n",
    "         _query_features = _query_features.unsqueeze(1)\n",
    "        \n",
    "    if isinstance(_query_features, Tensor): query_features = _query_features.numpy()\n",
    "    if isinstance(_index_features, Tensor): index_features = _index_features.numpy()\n",
    "    max_hits = 200\n",
    "    \n",
    "    n_patches = query_features.shape[1]\n",
    "    n_queries = query_features.shape[0]\n",
    "\n",
    "    query_features = query_features[:,::n_patches,:].squeeze(1).copy()\n",
    "    index_features = index_features[:,::n_patches,:].squeeze(1).copy()    \n",
    "    n_patches = 1\n",
    "\n",
    "    print(query_features.shape, index_features.shape, n_queries, n_patches)\n",
    "    \n",
    "    flat_config = faiss.GpuIndexFlatConfig()\n",
    "    flat_config.device = 0\n",
    "    res = faiss.StandardGpuResources()\n",
    "    co = faiss.GpuMultipleClonerOptions()\n",
    "    co.shard=True\n",
    "    co.shard_type=1\n",
    "    co.useFloat16=True\n",
    "    _index = faiss.index_factory(index_features.shape[1], index_type)#, faiss.METRIC_INNER_PRODUCT)\n",
    "    try:\n",
    "        faiss.index_cpu_to_all_gpus(_index,co=co)\n",
    "        print(\"Index in GPU\")\n",
    "    except:\n",
    "        index = _index\n",
    "        print(\"Index in CPU\")\n",
    "    print(\"Training index...\", end=\"\")\n",
    "    index.train(index_features)\n",
    "    print(\"done\")\n",
    "    print(\"Adding features to index...\", end=\"\")\n",
    "    index.add(index_features)\n",
    "    print(\"done\")\n",
    "    out_dists = np.zeros((len(query_features), max_hits), dtype=np.float32)\n",
    "    out_idxs  = np.zeros((len(query_features), max_hits), dtype=np.int32)\n",
    "    NUM_QUERY = len (query_features)\n",
    "    for ind in progress_bar(range(0, len(query_features), BS)):\n",
    "        fin = ind+BS\n",
    "        if fin > NUM_QUERY: fin = NUM_QUERY\n",
    "        q_descs = query_features[ind:fin]\n",
    "        D, I = index.search(q_descs, max_hits)\n",
    "        out_dists[ind:fin] = D\n",
    "        out_idxs[ind:fin] = I // n_patches\n",
    "    return out_idxs, out_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.omp_get_max_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type=f\"PCAW{query_features.shape[-1]},L2norm,Flat\"\n",
    "\n",
    "try:\n",
    "    out_idxs  = np.load(f'idx_{basename}.npy')\n",
    "    out_dists = np.load(f'dist_{basename}.npy')\n",
    "except:\n",
    "    out_idxs, out_dists = get_idxs_and_dists(\n",
    "        query_features.squeeze(1), \n",
    "        index_features.squeeze(1), BS = 32, index_type=index_type)\n",
    "    np.save(f'idx_{basename}.npy',  out_idxs)\n",
    "    np.save(f'dist_{basename}.npy', out_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(out_dists.reshape((-1,int(out_idxs.shape[1]*1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idxs.shape, out_dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fname = 'test_submission.csv'\n",
    "sample_df = pd.read_csv('/home/cvdcl/22521167/test.csv')\n",
    "sample_df['images'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = {}\n",
    "for i, query_fname in progress_bar(enumerate(query_fnames), total=len(query_fnames)):\n",
    "    if i % 2: continue\n",
    "    idx = np.concatenate([out_idxs[i], out_idxs[i+1]], axis=0)//2\n",
    "    dst = np.concatenate([out_dists[i],out_dists[i+1]], axis=0) \n",
    "    u_idx = np.unique(idx,return_index=True)[1]\n",
    "    i_dst = dst[u_idx]\n",
    "    o_dst =np.argsort(i_dst)\n",
    "    _out_idxs = idx[u_idx[o_dst]]\n",
    "\n",
    "    ids = [index_fnames[x] for x in _out_idxs[:100]]\n",
    "    sub[query_fname] = ' '.join(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra danh sách sub\n",
    "for query_fname, ids in sub.items():\n",
    "    print(f\"Query file: {query_fname}\")\n",
    "    print(f\"Top 100 nearest neighbors: {ids}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({'id' : list(sub.keys()), 'images':list(sub.values())})\n",
    "sub_df = pd.concat([sub_df, sample_df]).drop_duplicates(subset=['id'])\n",
    "sub_df.to_csv(sub_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.iloc[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_path(p):\n",
    "    fn = str(p.name)\n",
    "    return p.parent / fn[0] / fn[1] / fn[2] / fn\n",
    "\n",
    "def image_results(row, n=5):\n",
    "    # Sử dụng PILImage.create thay vì open_image, với 1 ảnh query và 5 ảnh result\n",
    "    r = [PILImage.create(fix_path(Path('test/test') / (row.id + '.jpg')))]  # Ảnh query\n",
    "    r.extend([PILImage.create(fix_path(Path('index/index') / (id + '.jpg'))) for id in row.images.split(' ')[:n]])  # 5 ảnh result\n",
    "    return r\n",
    "\n",
    "def show_all(images, r=1, figsize=(20, 10)):\n",
    "    # Hiển thị ảnh trên 1 hàng với 6 cột\n",
    "    c = len(images)  # Số cột chính bằng số ảnh\n",
    "    fig, axs = plt.subplots(r, c, figsize=figsize)  # Tạo lưới subplots với r hàng và c cột\n",
    "    axs = axs.flatten()  # Chuyển thành mảng 1 chiều để dễ truy cập\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ví dụ sử dụng show_all với dataframe\n",
    "show_all(image_results(sub_df.iloc[10]), r=1, figsize=(20, 10))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
